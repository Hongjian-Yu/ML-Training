{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e3f636-bea9-4518-9814-77a07efbe0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris,load_wine\n",
    "from DecisionTree import D_Tree \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from stratifion import stratified_sampling\n",
    "from MetaCost import Metacost\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from KNN_plus import KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f184d647-2887-4767-8688-bd50fc547423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cost_matrix(label):\n",
    "    # np.random.seed(0)\n",
    "    label_list,counts = np.unique(label,return_counts=True)\n",
    "    class_number = len(label_list)\n",
    "    data = np.zeros((class_number,class_number))\n",
    "    Data = pd.DataFrame(data,columns=label_list,index=label_list)\n",
    "    for name1 in label_list:\n",
    "        for name2 in label_list:\n",
    "            if name1 == name2:\n",
    "                Data[name1][name2] = np.random.randint(0,1000)\n",
    "            else:\n",
    "                Data[name1][name2] = np.random.randint(0,2000*counts[np.where(label_list==name1)] / counts[np.where(label_list==name2)])\n",
    "                # Data[name1][name2] = np.random.uniform(0,10000)\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a0ee41f-e8d9-4626-943d-745be116c0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        A      B      C\n",
      "A    35.0   77.0  901.0\n",
      "B  5050.0  875.0  746.0\n",
      "C  1898.0   86.0  834.0\n"
     ]
    }
   ],
   "source": [
    "print(make_cost_matrix(['A','A','B','C','A','C','A']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9da1ae64-66be-4102-8230-f2f5e8235c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cs(cost_matrix,true_label,predict_label):\n",
    "    cs = 0\n",
    "\n",
    "    for i in range(len(true_label)):\n",
    "        \n",
    "        # if true_label[i]!=predict_label[i]:\n",
    "        cs += cost_matrix.iloc[predict_label[i]][true_label[i]]\n",
    "\n",
    "    # conf_matrix = confusion_matrix(true_label, predict_label)\n",
    "    # total_cost = np.sum(conf_matrix * cost_matrix.values)\n",
    "\n",
    "    # return total_cost/len(true_label)\n",
    "    return cs/len(true_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "675d62cf-51e6-4293-90f7-7af7e9aea339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(true_label,predict_label):\n",
    "    acc = accuracy_score(true_label,predict_label)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52fc522b-6910-406a-8241-4c0b265c2340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_Auc(true_label,predict_label):\n",
    "#     return roc_auc_score(true_label,predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7c2a41c-9a38-49fc-bb1f-47077eb4d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(true_label,predict_label):\n",
    "    return f1_score(true_label,predict_label,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a2821e7-844b-46de-affe-2bc8f1e75eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C4.5 576.8511111111112\n",
      "C4.5 201.76813818139172\n",
      "under 476.83666666666676\n",
      "under 179.5797134841843\n",
      "over 497.2522222222222\n",
      "over 141.69888753843352\n",
      "meta 349.0966666666667\n",
      "meta 151.15693705059826\n",
      "my_algorithm 350.95666666666665\n",
      "my_algorithm 151.51289355149177\n"
     ]
    }
   ],
   "source": [
    "# iris数据集\n",
    "iris = load_iris()\n",
    "iris_data = iris.data\n",
    "iris_label = iris.target\n",
    "iris_ = pd.DataFrame(iris_data)\n",
    "column = iris_.columns\n",
    "iris_ = (iris_[column] - iris_[column].min()) / (iris_[column].max() - iris_[column].min())\n",
    "iris_['Y'] = iris_label\n",
    "cs = []\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(iris_label)\n",
    "    iris_train_set,iris_test_set = train_test_split(iris_,test_size=0.3,random_state=42)\n",
    "    iris_test_data = iris_test_set.iloc[:,:-1].values\n",
    "    iris_test_label = iris_test_set.iloc[:,-1].values\n",
    "    tree = D_Tree(iris_train_set)\n",
    "    tree.fit()\n",
    "    predict_label = tree.predict(iris_test_data)\n",
    "    cs.append(compute_cs(cost_matrix,iris_test_label,predict_label))\n",
    "cs = np.array(cs)\n",
    "# print(cs)\n",
    "print(\"C4.5\",np.mean(cs))\n",
    "print(\"C4.5\",np.std(cs))\n",
    "\n",
    "cs_ = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(iris_label)\n",
    "    iris_train_set,iris_test_set = train_test_split(iris_,test_size=0.3,random_state=42)\n",
    "    iris_test_data = iris_test_set.iloc[:,:-1].values\n",
    "    iris_test_label = iris_test_set.iloc[:,-1].values\n",
    "    train_set_x,train_set_y = stratified_sampling(iris_train_set,cost_matrix.values,'undersampling') \n",
    "    train_set = pd.DataFrame(train_set_x)\n",
    "    train_set['Y'] = train_set_y\n",
    "    tree = D_Tree(train_set)\n",
    "    tree.fit()\n",
    "    predict_label = tree.predict(iris_test_data)\n",
    "    cs_.append(compute_cs(cost_matrix,iris_test_label,predict_label))\n",
    "cs_ = np.array(cs_)\n",
    "# print(cs_)\n",
    "print(\"under\",np.mean(cs_))\n",
    "print(\"under\",np.std(cs_))\n",
    "\n",
    "cs__ = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(iris_label)\n",
    "    iris_train_set,iris_test_set = train_test_split(iris_,test_size=0.3,random_state=42)\n",
    "    iris_test_data = iris_test_set.iloc[:,:-1].values\n",
    "    iris_test_label = iris_test_set.iloc[:,-1].values\n",
    "    train_set_x,train_set_y = stratified_sampling(iris_train_set,cost_matrix.values,'oversampling') \n",
    "    train_set = pd.DataFrame(train_set_x)\n",
    "    train_set['Y'] = train_set_y\n",
    "    tree = D_Tree(train_set)\n",
    "    tree.fit()\n",
    "    predict_label = tree.predict(iris_test_data)\n",
    "    cs__.append(compute_cs(cost_matrix,iris_test_label,predict_label))\n",
    "cs__ = np.array(cs__)\n",
    "# print(cs__)\n",
    "print(\"over\",np.mean(cs__))\n",
    "print(\"over\",np.std(cs__))\n",
    "\n",
    "cs___=[]\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(iris_label)\n",
    "    iris_train_set,iris_test_set = train_test_split(iris_,test_size=0.3,random_state=42)\n",
    "    iris_test_data = iris_test_set.iloc[:,:-1].values\n",
    "    iris_test_label = iris_test_set.iloc[:,-1].values\n",
    "    tree = DecisionTreeClassifier(criterion='entropy')\n",
    "    meta = Metacost(iris_train_set,tree,cost_matrix.values,q=False)\n",
    "    model = meta.fit('Y')\n",
    "    predict_label = model.predict(iris_test_data)\n",
    "    cs___.append(compute_cs(cost_matrix,iris_test_label,predict_label))\n",
    "cs___ = np.array(cs___)\n",
    "# print(cs__)\n",
    "print(\"meta\",np.mean(cs___))\n",
    "print(\"meta\",np.std(cs___))\n",
    "\n",
    "cs_k = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(iris_label)\n",
    "    iris_train_set,iris_test_set = train_test_split(iris_,test_size=0.3,random_state=42)\n",
    "    iris_test_data = iris_test_set.iloc[:,:-1].values\n",
    "    iris_test_label = iris_test_set.iloc[:,-1].values\n",
    "    knn = KNN(2,iris_train_set,50)\n",
    "    knn.fit(cost_matrix.values,n_neighbor=20,euclidean_features=[0,1,2,3])\n",
    "    predict_label = knn.predict(iris_test_set)\n",
    "    cs_k.append(compute_cs(cost_matrix,iris_test_label,predict_label))\n",
    "cs_k = np.array(cs_k)\n",
    "print(\"my_algorithm\",np.mean(cs_k))\n",
    "print(\"my_algorithm\",np.std(cs_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e35a48af-7afd-4a7f-ac99-02285a165999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C4.5 584.4481481481481\n",
      "C4.5 112.32646188579307\n",
      "under 518.2370370370371\n",
      "under 127.281567322109\n",
      "over 536.4212962962963\n",
      "over 146.09400933513513\n",
      "meta 412.0768518518519\n",
      "meta 183.77790730612946\n",
      "my_algorithm 426.2990740740741\n",
      "my_algorithm 133.67565190942133\n"
     ]
    }
   ],
   "source": [
    "# Wine数据集\n",
    "wine = load_wine()\n",
    "wine_data = wine.data\n",
    "wine_label = wine.target\n",
    "wine_ = pd.DataFrame(wine_data)\n",
    "wine_['Y'] = wine_label\n",
    "cs = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(wine_label)\n",
    "\n",
    "    wine_train_set,wine_test_set = train_test_split(wine_,test_size=0.3)\n",
    "    wine_test_data = wine_test_set.iloc[:,:-1].values\n",
    "    wine_test_label = wine_test_set.iloc[:,-1].values\n",
    "    tree = D_Tree(wine_train_set)\n",
    "    tree.fit()\n",
    "    predict_label = tree.predict(wine_test_data)\n",
    "    cs.append(compute_cs(cost_matrix,wine_test_label,predict_label))\n",
    "cs = np.array(cs)\n",
    "print(\"C4.5\",cs.mean())\n",
    "print(\"C4.5\",cs.std())\n",
    "\n",
    "cs_ = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(wine_label)\n",
    "    wine_train_set,wine_test_set = train_test_split(wine_,test_size=0.3,random_state=42)\n",
    "    wine_test_data = wine_test_set.iloc[:,:-1].values\n",
    "    wine_test_label = wine_test_set.iloc[:,-1].values\n",
    "    train_set_x,train_set_y = stratified_sampling(wine_train_set,cost_matrix.values,'undersampling') \n",
    "    train_set = pd.DataFrame(train_set_x)\n",
    "    train_set['Y'] = train_set_y\n",
    "    tree = D_Tree(train_set)\n",
    "    tree.fit()\n",
    "    predict_label = tree.predict(wine_test_data)\n",
    "    cs_.append(compute_cs(cost_matrix,wine_test_label,predict_label))\n",
    "cs_ = np.array(cs_)\n",
    "# print(cs_)\n",
    "print(\"under\",np.mean(cs_))\n",
    "print(\"under\",np.std(cs_))\n",
    "\n",
    "cs__ = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(wine_label)\n",
    "    wine_train_set,wine_test_set = train_test_split(wine_,test_size=0.3,random_state=42)\n",
    "    wine_test_data = wine_test_set.iloc[:,:-1].values\n",
    "    wine_test_label = wine_test_set.iloc[:,-1].values\n",
    "    train_set_x,train_set_y = stratified_sampling(wine_train_set,cost_matrix.values,'oversampling') \n",
    "    train_set = pd.DataFrame(train_set_x)\n",
    "    train_set['Y'] = train_set_y\n",
    "    tree = D_Tree(train_set)\n",
    "    tree.fit()\n",
    "    predict_label = tree.predict(wine_test_data)\n",
    "    cs__.append(compute_cs(cost_matrix,wine_test_label,predict_label))\n",
    "cs__ = np.array(cs__)\n",
    "# print(cs__)\n",
    "print(\"over\",np.mean(cs__))\n",
    "print(\"over\",np.std(cs__))\n",
    "\n",
    "cs___=[]\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(wine_label)\n",
    "    wine_train_set,wine_test_set = train_test_split(wine_,test_size=0.3,random_state=42)\n",
    "    wine_test_data = wine_test_set.iloc[:,:-1].values\n",
    "    wine_test_label = wine_test_set.iloc[:,-1].values\n",
    "    tree = DecisionTreeClassifier(criterion='entropy')\n",
    "    meta = Metacost(wine_train_set,tree,cost_matrix.values,q=False)\n",
    "    model = meta.fit('Y')\n",
    "    predict_label = model.predict(wine_test_data)\n",
    "    cs___.append(compute_cs(cost_matrix,wine_test_label,predict_label))\n",
    "cs___ = np.array(cs___)\n",
    "# print(cs__)\n",
    "print(\"meta\",np.mean(cs___))\n",
    "print(\"meta\",np.std(cs___))\n",
    "\n",
    "cs_k = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(wine_label)\n",
    "    wine_train_set,wine_test_set = train_test_split(wine_,test_size=0.3,random_state=42)\n",
    "    wine_test_data = wine_test_set.iloc[:,:-1].values\n",
    "    wine_test_label = wine_test_set.iloc[:,-1].values\n",
    "    knn = KNN(3,wine_train_set,50)\n",
    "    knn.fit(cost_matrix.values,n_neighbor=20,euclidean_features=[i for i in range(wine_train_set.shape[1]-1)])\n",
    "    predict_label = knn.predict(wine_test_set)\n",
    "    cs_k.append(compute_cs(cost_matrix,wine_test_label,predict_label))\n",
    "cs_k = np.array(cs_k)\n",
    "print(\"my_algorithm\",np.mean(cs_k))\n",
    "print(\"my_algorithm\",np.std(cs_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37041705-f182-4d4b-b1f7-e4120be3b831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C4.5: 540.2409259259259\n",
      "C4.5: 202.40074530893358\n"
     ]
    }
   ],
   "source": [
    "annealing = fetch_ucirepo(id=3) \n",
    "anneal_data = annealing.data.features.values\n",
    "anneal_label = annealing.data.targets.values\n",
    "\n",
    "unique = np.unique(anneal_label)\n",
    "\n",
    "change = {unique[i]:i for i in range(len(unique))}\n",
    "\n",
    "a = []\n",
    "for i in anneal_label:\n",
    "\n",
    "    a.append(int(change[i[0]]))\n",
    "anneal_label = a\n",
    "\n",
    "anneal_ = pd.DataFrame(anneal_data)\n",
    "anneal_[38] = anneal_label\n",
    "ann = anneal_\n",
    "\n",
    "colum =  anneal_.columns\n",
    "\n",
    "\n",
    "fea_con = [3,4,8,32,33,34,38]\n",
    "fea_els = [i for i in colum if i not in fea_con]\n",
    "\n",
    "anneal_ = anneal_[colum[fea_els]].astype(str)\n",
    "anneal_ = pd.get_dummies(anneal_)\n",
    "anneal_[colum[fea_con]] = ann[colum[fea_con]].values\n",
    "cs = []\n",
    "for j in range(20):\n",
    "    cost_matrix = make_cost_matrix(anneal_label)\n",
    "    anneal_train_set,anneal_test_set = train_test_split(anneal_,test_size=0.3)\n",
    "    anneal_test_data = anneal_test_set.iloc[:,:-1].values\n",
    "    anneal_test_label = anneal_test_set.iloc[:,-1].values\n",
    "    \n",
    "    t_data = anneal_train_set.iloc[:,:-1].values\n",
    "    t_label = anneal_train_set.iloc[:,-1].values\n",
    "    t_label = [int(i) for i in t_label]\n",
    "    tree = DecisionTreeClassifier(criterion='entropy')\n",
    "    tree.fit(t_data,t_label)\n",
    "    \n",
    "    predict_label = tree.predict(anneal_test_data)\n",
    "    \n",
    "    cs.append(compute_cs(cost_matrix,anneal_test_label,predict_label))\n",
    "print(\"C4.5:\",np.mean(cs))\n",
    "print(\"C4.5:\",np.std(cs))\n",
    "\n",
    "# cs_=[]\n",
    "# for j in range(20):\n",
    "#     cost_matrix = make_cost_matrix(anneal_label)\n",
    "#     anneal_train_set,anneal_test_set = train_test_split(anneal_,test_size=0.3)\n",
    "#     anneal_test_data = anneal_test_set.iloc[:,:-1].values\n",
    "#     anneal_test_label = anneal_test_set.iloc[:,-1].values\n",
    "#     t_data,t_label = stratified_sampling(anneal_train_set,cost_matrix,'undersampling')\n",
    "#     t_label = [int(i) for i in t_label]\n",
    "#     tree = DecisionTreeClassifier(criterion='entropy')\n",
    "#     tree.fit(t_data,t_label)\n",
    "#     predict_label = tree.predict(anneal_test_data)\n",
    "#     cs_.append(compute_cs(cost_matrix,anneal_test_label,predict_label))\n",
    "# print(\"undersampling:\",np.mean(cs_))\n",
    "# print(\"undersamping\",np.std(cs_))\n",
    "\n",
    "\n",
    "# cs__=[]\n",
    "# for j in range(20):\n",
    "#     cost_matrix = make_cost_matrix(anneal_label)\n",
    "#     anneal_train_set,anneal_test_set = train_test_split(anneal_,test_size=0.3)\n",
    "#     anneal_test_data = anneal_test_set.iloc[:,:-1].values\n",
    "#     anneal_test_label = anneal_test_set.iloc[:,-1].values\n",
    "#     t_data,t_label = stratified_sampling(anneal_train_set,cost_matrix,'oversampling')\n",
    "#     t_label = [int(i) for i in t_label]\n",
    "#     tree = DecisionTreeClassifier(criterion='entropy')\n",
    "#     tree.fit(t_data,t_label)\n",
    "#     predict_label = tree.predict(anneal_test_data)\n",
    "#     cs__.append(compute_cs(cost_matrix,anneal_test_label,predict_label))\n",
    "# print(\"oversampling:\",np.mean(cs__))\n",
    "# print(\"voersamping\",np.std(cs__))\n",
    "\n",
    "\n",
    "# css_=[]\n",
    "# for j in range(20):\n",
    "#     cost_matrix = make_cost_matrix(anneal_label)\n",
    "#     anneal_train_set,anneal_test_set = train_test_split(anneal_,test_size=0.3)\n",
    "#     # print(anneal_train_set)\n",
    "#     anneal_test_data = anneal_test_set.iloc[:,:-1].values\n",
    "#     anneal_test_label = anneal_test_set.iloc[:,-1].values\n",
    "#     tree = DecisionTreeClassifier(criterion='entropy')\n",
    "#     meta = Metacost(anneal_train_set,tree,cost_matrix.values,q=False)\n",
    "#     new_model = meta.fit(38)\n",
    "#     predict_label  = new_model.predict(anneal_test_data)\n",
    "#     css_.append(compute_cs(cost_matrix,anneal_test_label,predict_label))\n",
    "# print(\"meta\",np.mean(css_))\n",
    "# print(\"meta\",np.std(css_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e9f9ad4-4dc5-48fb-baba-a46bb9b13016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C4.5 989.96875\n",
      "C4.5 502.9256342191035\n",
      "under 702.4625\n",
      "under 397.97504456152774\n",
      "over 650.3625\n",
      "over 353.98293104152634\n",
      "meta 656.00625\n",
      "meta 601.0089556776068\n",
      "my_algorithm 408.73125\n",
      "my_algorithm 117.89989314960171\n"
     ]
    }
   ],
   "source": [
    "lenses = fetch_ucirepo(id=58) \n",
    "X = lenses.data.features.values\n",
    "y = lenses.data.targets.values\n",
    "unique = np.unique(y,return_counts=False)\n",
    "change = {unique[i]:i for i in range(len(unique))}\n",
    "# print(y)\n",
    "y = [change[i[0]] for i in y]\n",
    "lenses_ = pd.DataFrame(X)\n",
    "lenses_['Y'] = y\n",
    "\n",
    "cs = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    lenses_train_set,lenses_test_set = train_test_split(lenses_,test_size=0.3)\n",
    "    lenses_test_data = lenses_test_set.iloc[:,:-1].values\n",
    "    lenses_test_label = lenses_test_set.iloc[:,-1].values\n",
    "    tree = D_Tree(lenses_train_set)\n",
    "    tree.fit()\n",
    "    predict_label = tree.predict(lenses_test_data)\n",
    "    cs.append(compute_cs(cost_matrix,lenses_test_label,predict_label))\n",
    "cs = np.array(cs)\n",
    "print(\"C4.5\",cs.mean())\n",
    "print(\"C4.5\",cs.std())\n",
    "\n",
    "cs_ = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    lenses_train_set,lenses_test_set = train_test_split(lenses_,test_size=0.3)\n",
    "    lenses_test_data = lenses_test_set.iloc[:,:-1].values\n",
    "    lenses_test_label = lenses_test_set.iloc[:,-1].values\n",
    "    train_set_x,train_set_y = stratified_sampling(lenses_train_set,cost_matrix.values,'undersampling') \n",
    "    train_set = pd.DataFrame(train_set_x)\n",
    "    train_set['Y'] = train_set_y\n",
    "    tree = D_Tree(train_set)\n",
    "    tree.fit()\n",
    "    predict_label = tree.predict(lenses_test_data)\n",
    "    cs_.append(compute_cs(cost_matrix,lenses_test_label,predict_label))\n",
    "cs_ = np.array(cs_)\n",
    "# print(cs_)\n",
    "print(\"under\",np.mean(cs_))\n",
    "print(\"under\",np.std(cs_))\n",
    "\n",
    "cs__ = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    lenses_train_set,lenses_test_set = train_test_split(lenses_,test_size=0.3)\n",
    "    lenses_test_data = lenses_test_set.iloc[:,:-1].values\n",
    "    lenses_test_label = lenses_test_set.iloc[:,-1].values\n",
    "    train_set_x,train_set_y = stratified_sampling(lenses_train_set,cost_matrix.values,'oversampling') \n",
    "    train_set = pd.DataFrame(train_set_x)\n",
    "    train_set['Y'] = train_set_y\n",
    "    tree = D_Tree(train_set)\n",
    "    tree.fit()\n",
    "    predict_label = tree.predict(lenses_test_data)\n",
    "    cs__.append(compute_cs(cost_matrix,lenses_test_label,predict_label))\n",
    "cs__ = np.array(cs__)\n",
    "# print(cs_)\n",
    "print(\"over\",np.mean(cs__))\n",
    "print(\"over\",np.std(cs__))\n",
    "\n",
    "cs___=[]\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    lenses_train_set,wine_test_set = train_test_split(lenses_,test_size=0.3,random_state=42)\n",
    "    lenses_test_data = lenses_test_set.iloc[:,:-1].values\n",
    "    lenses_test_label = lenses_test_set.iloc[:,-1].values\n",
    "    tree = DecisionTreeClassifier(criterion='entropy')\n",
    "    meta = Metacost(lenses_train_set,tree,cost_matrix.values,q=False)\n",
    "    model = meta.fit('Y')\n",
    "    predict_label = model.predict(lenses_test_data)\n",
    "    cs___.append(compute_cs(cost_matrix,lenses_test_label,predict_label))\n",
    "cs___ = np.array(cs___)\n",
    "# print(cs__)\n",
    "print(\"meta\",np.mean(cs___))\n",
    "print(\"meta\",np.std(cs___))\n",
    "\n",
    "cs_k = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    lenses_train_set,lenses_test_set = train_test_split(lenses_,test_size=0.3,random_state=42)\n",
    "    lenses_test_data = lenses_test_set.iloc[:,:-1].values\n",
    "    lenses_test_label = lenses_test_set.iloc[:,-1].values\n",
    "    knn = KNN(2,lenses_train_set,50)\n",
    "    knn.fit(cost_matrix.values,n_neighbor=6,hamming_features=[i for i in range(lenses_train_set.shape[1]-1)])\n",
    "    predict_label = knn.predict(wine_test_set)\n",
    "    cs_k.append(compute_cs(cost_matrix,lenses_test_label,predict_label))\n",
    "cs_k = np.array(cs_k)\n",
    "print(\"my_algorithm\",np.mean(cs_k))\n",
    "print(\"my_algorithm\",np.std(cs_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "580cf7f2-87a6-4dc8-879c-2996cea10b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C4.5 488.63\n",
      "C4.5 129.07831640777883\n",
      "under 447.74000000000007\n",
      "under 168.40027962499877\n",
      "over 530.7500000000001\n",
      "over 163.56609792034806\n",
      "meta 391.4733333333333\n",
      "meta 105.7001353935851\n",
      "my_algorithm 560.79\n",
      "my_algorithm 215.110478359377\n"
     ]
    }
   ],
   "source": [
    "# fetch dataset \n",
    "soybean_small = fetch_ucirepo(id=91) \n",
    "X = soybean_small.data.features.values \n",
    "y = soybean_small.data.targets.values\n",
    "unique = np.unique(y,return_counts=False)\n",
    "change = {unique[i]:i for i in range(len(unique))}\n",
    "y = [change[i[0]] for i in y]\n",
    "soybean_ = pd.DataFrame(X)\n",
    "soybean_['Y'] = y\n",
    "\n",
    "\n",
    "cs = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    soybean_train_set,soybean_test_set = train_test_split(soybean_,test_size=0.3)\n",
    "    soybean_test_data = soybean_test_set.iloc[:,:-1].values\n",
    "    soybean_test_label = soybean_test_set.iloc[:,-1].values\n",
    "    tree = D_Tree(soybean_train_set)\n",
    "    tree.fit()\n",
    "    predict_label = tree.predict(soybean_test_data)\n",
    "    cs.append(compute_cs(cost_matrix,soybean_test_label,predict_label))\n",
    "cs = np.array(cs)\n",
    "print(\"C4.5\",cs.mean())\n",
    "print(\"C4.5\",cs.std())\n",
    "\n",
    "cs_ = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    soybean_train_set,soybean_test_set = train_test_split(soybean_,test_size=0.3)\n",
    "    soybean_test_data = soybean_test_set.iloc[:,:-1].values\n",
    "    soybean_test_label = soybean_test_set.iloc[:,-1].values\n",
    "    train_set_x,train_set_y = stratified_sampling(soybean_train_set,cost_matrix.values,'undersampling') \n",
    "    train_set = pd.DataFrame(train_set_x)\n",
    "    train_set['Y'] = train_set_y\n",
    "    tree = D_Tree(train_set)\n",
    "    tree.fit()\n",
    "    predict_label = tree.predict(soybean_test_data)\n",
    "    cs_.append(compute_cs(cost_matrix,soybean_test_label,predict_label))\n",
    "cs_ = np.array(cs_)\n",
    "print(\"under\",cs_.mean())\n",
    "print(\"under\",cs_.std())\n",
    "\n",
    "cs__ = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    soybean_train_set,soybean_test_set = train_test_split(soybean_,test_size=0.3)\n",
    "    soybean_test_data = soybean_test_set.iloc[:,:-1].values\n",
    "    soybean_test_label = soybean_test_set.iloc[:,-1].values\n",
    "    train_set_x,train_set_y = stratified_sampling(soybean_train_set,cost_matrix.values,'oversampling') \n",
    "    train_set = pd.DataFrame(train_set_x)\n",
    "    train_set['Y'] = train_set_y\n",
    "    tree = D_Tree(train_set)\n",
    "    tree.fit()\n",
    "    predict_label = tree.predict(soybean_test_data)\n",
    "    cs__.append(compute_cs(cost_matrix,soybean_test_label,predict_label))\n",
    "cs__ = np.array(cs__)\n",
    "print(\"over\",cs__.mean())\n",
    "print(\"over\",cs__.std())\n",
    "\n",
    "\n",
    "cs___=[]\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    soybean_train_set,soybean_test_set = train_test_split(soybean_,test_size=0.3,random_state=42)\n",
    "    soybean_test_data = soybean_test_set.iloc[:,:-1].values\n",
    "    soybean_test_label = soybean_test_set.iloc[:,-1].values\n",
    "    tree = DecisionTreeClassifier(criterion='entropy')\n",
    "    meta = Metacost(soybean_train_set,tree,cost_matrix.values,q=False)\n",
    "    model = meta.fit('Y')\n",
    "    predict_label = model.predict(soybean_test_data)\n",
    "    cs___.append(compute_cs(cost_matrix,soybean_test_label,predict_label))\n",
    "cs___ = np.array(cs___)\n",
    "# print(cs__)\n",
    "print(\"meta\",np.mean(cs___))\n",
    "print(\"meta\",np.std(cs___))\n",
    "\n",
    "cs_k = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    soybean_train_set,soybean_test_set = train_test_split(soybean_,test_size=0.3,random_state=42)\n",
    "    soybean_test_data = soybean_test_set.iloc[:,:-1].values\n",
    "    soybean_test_label = soybean_test_set.iloc[:,-1].values\n",
    "    knn = KNN(2,soybean_train_set,50)\n",
    "    knn.fit(cost_matrix.values,n_neighbor=6,hamming_features=[i for i in range(soybean_train_set.shape[1]-1)])\n",
    "    predict_label = knn.predict(soybean_test_set)\n",
    "    cs_k.append(compute_cs(cost_matrix,soybean_test_label,predict_label))\n",
    "cs_k = np.array(cs_k)\n",
    "print(\"my_algorithm\",np.mean(cs_k))\n",
    "print(\"my_algorithm\",np.std(cs_k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afe98e6a-4415-4111-8ad1-7b20255e4c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C4.5 807.835\n",
      "C4.5 270.23318129904027\n",
      "undersampling 745.3599999999999\n",
      "undersampling 178.04232755162465\n",
      "oversampling 831.6\n",
      "oversampling 271.47579265930875\n",
      "meta 632.88\n",
      "meta 284.28659412642025\n",
      "my_algorithm 605.66\n",
      "my_algorithm 286.79842468186604\n"
     ]
    }
   ],
   "source": [
    "lung_cancer = fetch_ucirepo(id=62) \n",
    "X = lung_cancer.data.features.values\n",
    "y = lung_cancer.data.targets.values\n",
    "unique = np.unique(y,return_counts=False)\n",
    "change = {unique[i]:i for i in range(len(unique))}\n",
    "y = [change[i[0]] for i in y]\n",
    "lung_ = pd.DataFrame(X)\n",
    "lung_[56] = y\n",
    "lun = lung_\n",
    "\n",
    "\n",
    "fea_cate = [3,37]\n",
    "fea_con = [i for i in range(lung_.shape[1]) if i not in fea_cate]\n",
    "\n",
    "lung_ = lung_[fea_cate].astype(str)\n",
    "lung_ = pd.get_dummies(lung_)\n",
    "index = [i for i in range(lung_.shape[1],lung_.shape[1]+len(fea_con))]\n",
    "\n",
    "lung_[index] = lun[fea_con].values\n",
    "\n",
    "lung_ = lung_.rename(columns={62:'Y'})\n",
    "\n",
    "cs = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    lung_train_set,lung_test_set = train_test_split(lung_,test_size=0.3)\n",
    "    lung_test_data = lung_test_set.iloc[:,:-1].values\n",
    "    lung_test_label = lung_test_set.iloc[:,-1].values\n",
    "    tree = D_Tree(lung_train_set)\n",
    "    tree.fit()\n",
    "    predict_label = tree.predict(lung_test_data)\n",
    "    predict_label = [int(i) for i in predict_label]\n",
    "    cs.append(compute_cs(cost_matrix,lung_test_label,predict_label))\n",
    "cs = np.array(cs)\n",
    "print(\"C4.5\",cs.mean())\n",
    "print(\"C4.5\",cs.std())\n",
    "\n",
    "cs_ = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    lung_train_set,lung_test_set = train_test_split(lung_,test_size=0.3)\n",
    "    lung_test_data = lung_test_set.iloc[:,:-1].values\n",
    "    lung_test_label = lung_test_set.iloc[:,-1].values\n",
    "    train_set_x,train_set_y = stratified_sampling(lung_train_set,cost_matrix.values,'undersampling') \n",
    "    train_set = pd.DataFrame(train_set_x)\n",
    "    train_set['Y'] = train_set_y\n",
    "    tree = D_Tree(train_set)\n",
    "    tree.fit()\n",
    "    predict_label = tree.predict(lung_test_data)\n",
    "    predict_label = [int(i) for i in predict_label]\n",
    "    cs_.append(compute_cs(cost_matrix,lung_test_label,predict_label))\n",
    "cs_ = np.array(cs_)\n",
    "print(\"undersampling\",cs_.mean())\n",
    "print(\"undersampling\",cs_.std())\n",
    "\n",
    "cs__ = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    lung_train_set,lung_test_set = train_test_split(lung_,test_size=0.3)\n",
    "    lung_test_data = lung_test_set.iloc[:,:-1].values\n",
    "    lung_test_label = lung_test_set.iloc[:,-1].values\n",
    "    train_set_x,train_set_y = stratified_sampling(lung_train_set,cost_matrix.values,'oversampling') \n",
    "    train_set = pd.DataFrame(train_set_x)\n",
    "    train_set['Y'] = train_set_y\n",
    "    tree = D_Tree(train_set)\n",
    "    tree.fit()\n",
    "    predict_label = tree.predict(lung_test_data)\n",
    "    predict_label = [int(i) for i in predict_label]\n",
    "    cs__.append(compute_cs(cost_matrix,lung_test_label,predict_label))\n",
    "cs__ = np.array(cs__)\n",
    "print(\"oversampling\",cs__.mean())\n",
    "print(\"oversampling\",cs__.std())\n",
    "\n",
    "\n",
    "cs___=[]\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    lung_train_set,lung_test_set = train_test_split(lung_,test_size=0.3,random_state=42)\n",
    "    lung_test_data = lung_test_set.iloc[:,:-1].values\n",
    "    lung_test_label = lung_test_set.iloc[:,-1].values\n",
    "    tree = DecisionTreeClassifier(criterion='entropy')\n",
    "    meta = Metacost(lung_train_set,tree,cost_matrix.values,q=False)\n",
    "    model = meta.fit('Y')\n",
    "    predict_label = model.predict(lung_test_data)\n",
    "    predict_label = [int(i) for i in predict_label]\n",
    "    cs___.append(compute_cs(cost_matrix,lung_test_label,predict_label))\n",
    "cs___ = np.array(cs___)\n",
    "# print(cs__)\n",
    "print(\"meta\",np.mean(cs___))\n",
    "print(\"meta\",np.std(cs___))\n",
    "\n",
    "cs_k = []\n",
    "lun = lun.replace(np.nan,1000)\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    lung_train_set,lung_test_set = train_test_split(lun,test_size=0.3,random_state=42)\n",
    "    lung_test_data = lung_test_set.iloc[:,:-1].values\n",
    "    lung_test_label = lung_test_set.iloc[:,-1].values\n",
    "    knn = KNN(2,lung_train_set,50)\n",
    "    knn.fit(cost_matrix.values,n_neighbor=6,hamming_features=[i for i in range(lung_train_set.shape[1]-1)])\n",
    "    predict_label = knn.predict(lung_test_set)\n",
    "    cs_k.append(compute_cs(cost_matrix,lung_test_label,predict_label))\n",
    "cs_k = np.array(cs_k)\n",
    "print(\"my_algorithm\",np.mean(cs_k))\n",
    "print(\"my_algorithm\",np.std(cs_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee7ec43a-5551-4117-ad5c-c551df4d4b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C4.5 848.2978723404254\n",
      "C4.5 381.4059556270422\n",
      "undersampling 526.9606382978724\n",
      "undersampling 270.3064084913273\n",
      "meta 435.2329787234042\n",
      "meta 258.55962424195206\n"
     ]
    }
   ],
   "source": [
    "hepatitis = fetch_ucirepo(id=46) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = hepatitis.data.features.values \n",
    "y = hepatitis.data.targets.values\n",
    "unique = np.unique(y,return_counts=False)\n",
    "change = {unique[i]:i for i in range(len(unique))}\n",
    "y = [change[i[0]] for i in y]\n",
    "hep_ = pd.DataFrame(X)\n",
    "hep_[19] = y\n",
    "hep = hep_\n",
    "\n",
    "fea_cate = [2,4,5,6,7,8,9,10,11,12,13,14,15,16,17]\n",
    "fea_con = [i for i in range(hep_.shape[1]) if i not in fea_cate]\n",
    "\n",
    "hep_ = hep_[fea_cate].astype(str)\n",
    "hep_ = pd.get_dummies(hep_)\n",
    "index = [i for i in range(hep_.shape[1],hep_.shape[1]+len(fea_con))]\n",
    "\n",
    "hep_[index] = hep[fea_con].values\n",
    "\n",
    "hep_ = hep_.rename(columns={hep_.shape[1]-1:'Y'})\n",
    "\n",
    "\n",
    "cs = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    hep_train_set,hep_test_set = train_test_split(hep_,test_size=0.3)\n",
    "    hep_test_data = hep_test_set.iloc[:,:-1].values\n",
    "    hep_test_label = hep_test_set.iloc[:,-1].values\n",
    "    tree = D_Tree(hep_train_set)\n",
    "    tree.fit()\n",
    "    predict_label = tree.predict(hep_test_data)\n",
    "    predict_label = [int(i) for i in predict_label]\n",
    "    cs.append(compute_cs(cost_matrix,hep_test_label,predict_label))\n",
    "cs = np.array(cs)\n",
    "print(\"C4.5\",cs.mean())\n",
    "print(\"C4.5\",cs.std())\n",
    "\n",
    "cs_ = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    hep_train_set,hep_test_set = train_test_split(hep_,test_size=0.3)\n",
    "    hep_test_data = hep_test_set.iloc[:,:-1].values\n",
    "    hep_test_label = hep_test_set.iloc[:,-1].values\n",
    "    train_set_x,train_set_y = stratified_sampling(hep_train_set,cost_matrix.values,'undersampling') \n",
    "    train_set = pd.DataFrame(train_set_x)\n",
    "    train_set['Y'] = train_set_y\n",
    "    tree = D_Tree(train_set)\n",
    "    tree.fit()\n",
    "    predict_label = tree.predict(hep_test_data)\n",
    "    predict_label = [int(i) for i in predict_label]\n",
    "    cs_.append(compute_cs(cost_matrix,hep_test_label,predict_label))\n",
    "cs_ = np.array(cs_)\n",
    "print(\"undersampling\",cs_.mean())\n",
    "print(\"undersampling\",cs_.std())\n",
    "\n",
    "# cs__ = []\n",
    "# for i in range(20):\n",
    "#     cost_matrix = make_cost_matrix(y)\n",
    "#     hep_train_set,hep_test_set = train_test_split(hep_,test_size=0.3)\n",
    "#     hep_test_data = hep_test_set.iloc[:,:-1].values\n",
    "#     hep_test_label = hep_test_set.iloc[:,-1].values\n",
    "#     train_set_x,train_set_y = stratified_sampling(hep_train_set,cost_matrix.values,'oversampling') \n",
    "#     train_set = pd.DataFrame(train_set_x)\n",
    "#     train_set['Y'] = train_set_y\n",
    "#     tree = D_Tree(train_set)\n",
    "#     tree.fit()\n",
    "#     predict_label = tree.predict(hep_test_data)\n",
    "#     predict_label = [int(i) for i in predict_label]\n",
    "#     cs__.append(compute_cs(cost_matrix,hep_test_label,predict_label))\n",
    "# cs__ = np.array(cs__)\n",
    "# print(\"oversampling\",cs__.mean())\n",
    "# print(\"oversampling\",cs__.std())\n",
    "\n",
    "cs___=[]\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    hep_train_set,lung_test_set = train_test_split(hep_,test_size=0.3,random_state=42)\n",
    "    hep_test_data = hep_test_set.iloc[:,:-1].values\n",
    "    hep_test_label = hep_test_set.iloc[:,-1].values\n",
    "    tree = DecisionTreeClassifier(criterion='entropy')\n",
    "    meta = Metacost(hep_train_set,tree,cost_matrix.values,q=False)\n",
    "    model = meta.fit('Y')\n",
    "    predict_label = model.predict(hep_test_data)\n",
    "    predict_label = [int(i) for i in predict_label]\n",
    "    cs___.append(compute_cs(cost_matrix,hep_test_label,predict_label))\n",
    "cs___ = np.array(cs___)\n",
    "# print(cs__)\n",
    "print(\"meta\",np.mean(cs___))\n",
    "print(\"meta\",np.std(cs___))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06bc96fe-d5d6-4fcf-ab47-3415bc58ee48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C4.5 626.8174603174602\n",
      "C4.5 221.98768328867257\n",
      "under 654.6293650793651\n",
      "under 158.7025545263123\n",
      "under 521.8738095238095\n",
      "under 196.79705584969017\n",
      "meta 592.9198412698412\n",
      "meta 220.38655539486936\n",
      "my_algorithm 577.0761904761905\n",
      "my_algorithm 268.7980790417749\n"
     ]
    }
   ],
   "source": [
    "connectionist_bench_sonar_mines_vs_rocks = fetch_ucirepo(id=151) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = connectionist_bench_sonar_mines_vs_rocks.data.features.values \n",
    "y = connectionist_bench_sonar_mines_vs_rocks.data.targets.values\n",
    "\n",
    "unique = np.unique(y,return_counts=False)\n",
    "change = {unique[i]:i for i in range(len(unique))}\n",
    "y = [change[i[0]] for i in y]\n",
    "sonar_ = pd.DataFrame(X)\n",
    "sonar_['Y'] = y\n",
    "\n",
    "\n",
    "cs = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    sonar_train_set,sonar_test_set = train_test_split(sonar_,test_size=0.3)\n",
    "    sonar_test_data = sonar_test_set.iloc[:,:-1].values\n",
    "    sonar_test_label = sonar_test_set.iloc[:,-1].values\n",
    "    tree = D_Tree(sonar_train_set)\n",
    "    tree.fit()\n",
    "    predict_label = tree.predict(sonar_test_data)\n",
    "    cs.append(compute_cs(cost_matrix,sonar_test_label,predict_label))\n",
    "cs = np.array(cs)\n",
    "print(\"C4.5\",cs.mean())\n",
    "print(\"C4.5\",cs.std())\n",
    "\n",
    "cs_ = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    sonar_train_set,sonar_test_set = train_test_split(sonar_,test_size=0.3)\n",
    "    sonar_test_data = sonar_test_set.iloc[:,:-1].values\n",
    "    sonar_test_label = sonar_test_set.iloc[:,-1].values\n",
    "    train_set_x,train_set_y = stratified_sampling(sonar_train_set,cost_matrix.values,'undersampling') \n",
    "    train_set = pd.DataFrame(train_set_x)\n",
    "    train_set['Y'] = train_set_y\n",
    "    tree = D_Tree(train_set)\n",
    "    tree.fit()\n",
    "    predict_label = tree.predict(sonar_test_data)\n",
    "    cs_.append(compute_cs(cost_matrix,sonar_test_label,predict_label))\n",
    "cs_ = np.array(cs_)\n",
    "print(\"under\",cs_.mean())\n",
    "print(\"under\",cs_.std())\n",
    "\n",
    "cs__ = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    sonar_train_set,soybean_test_set = train_test_split(sonar_,test_size=0.3)\n",
    "    sonar_test_data = sonar_test_set.iloc[:,:-1].values\n",
    "    sonar_test_label = sonar_test_set.iloc[:,-1].values\n",
    "    train_set_x,train_set_y = stratified_sampling(sonar_train_set,cost_matrix.values,'oversampling') \n",
    "    train_set = pd.DataFrame(train_set_x)\n",
    "    train_set['Y'] = train_set_y\n",
    "    tree = D_Tree(train_set)\n",
    "    tree.fit()\n",
    "    predict_label = tree.predict(sonar_test_data)\n",
    "    cs__.append(compute_cs(cost_matrix,sonar_test_label,predict_label))\n",
    "cs__ = np.array(cs__)\n",
    "print(\"under\",cs__.mean())\n",
    "print(\"under\",cs__.std())\n",
    "\n",
    "\n",
    "cs___=[]\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    sonar_train_set,sonar_test_set = train_test_split(sonar_,test_size=0.3,random_state=42)\n",
    "    sonar_test_data = sonar_test_set.iloc[:,:-1].values\n",
    "    sonar_test_label = sonar_test_set.iloc[:,-1].values\n",
    "    tree = DecisionTreeClassifier(criterion='entropy')\n",
    "    meta = Metacost(sonar_train_set,tree,cost_matrix.values,q=False)\n",
    "    model = meta.fit('Y')\n",
    "    predict_label = model.predict(sonar_test_data)\n",
    "    cs___.append(compute_cs(cost_matrix,sonar_test_label,predict_label))\n",
    "cs___ = np.array(cs___)\n",
    "# print(cs__)\n",
    "print(\"meta\",np.mean(cs___))\n",
    "print(\"meta\",np.std(cs___))\n",
    "\n",
    "# lun = lun.replace(np.nan,1000)\n",
    "cs_k = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    sonar_train_set,sonar_test_set = train_test_split(sonar_,test_size=0.3,random_state=42)\n",
    "    sonar_test_data = sonar_test_set.iloc[:,:-1].values\n",
    "    sonar_test_label = sonar_test_set.iloc[:,-1].values\n",
    "    knn = KNN(6,sonar_train_set,50)\n",
    "    knn.fit(cost_matrix.values,n_neighbor=20,euclidean_features=[i for i in range(sonar_train_set.shape[1]-1)])\n",
    "    predict_label = knn.predict(sonar_test_set)\n",
    "    cs_k.append(compute_cs(cost_matrix,sonar_test_label,predict_label))\n",
    "cs_k = np.array(cs_k)\n",
    "print(\"my_algorithm\",np.mean(cs_k))\n",
    "print(\"my_algorithm\",np.std(cs_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ecd60e-60d2-48f5-b420-b38d6a5649c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a67c17cc-78e8-4599-978c-4280767a90c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C4.5 739.0515151515151\n",
      "C4.5 169.54476287548417\n",
      "under 594.2123376623377\n",
      "under 149.96204061385873\n",
      "over 433.3532467532467\n",
      "over 177.9404864472333\n",
      "meta 489.28744588744587\n",
      "meta 168.4466260896014\n"
     ]
    }
   ],
   "source": [
    "pima = pd.read_csv('pima.dat',header=None)\n",
    "x = pima.iloc[:,:-1].values\n",
    "y = pima.iloc[:,-1].values\n",
    "unique = np.unique(y)\n",
    "change = {unique[i]:i for i in range(len(unique))}\n",
    "y = [change[i] for i in y]\n",
    "pima_ = pd.DataFrame(x)\n",
    "pima_['Y'] = y\n",
    "\n",
    "cs = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    pima_train_set,pima_test_set = train_test_split(pima_,test_size=0.3)\n",
    "    pima_test_data = pima_test_set.iloc[:,:-1].values\n",
    "    pima_test_label = pima_test_set.iloc[:,-1].values\n",
    "    tree = D_Tree(pima_train_set)\n",
    "    tree.fit()\n",
    "    predict_label = tree.predict(pima_test_data)\n",
    "    \n",
    "    cs.append(compute_cs(cost_matrix,pima_test_label,predict_label))\n",
    "cs = np.array(cs)\n",
    "print(\"C4.5\",cs.mean())\n",
    "print(\"C4.5\",cs.std())\n",
    "\n",
    "cs_ = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    pima_train_set,sonar_test_set = train_test_split(pima_,test_size=0.3)\n",
    "    pima_test_data = pima_test_set.iloc[:,:-1].values\n",
    "    pima_test_label = pima_test_set.iloc[:,-1].values\n",
    "    train_set_x,train_set_y = stratified_sampling(pima_train_set,cost_matrix.values,'undersampling') \n",
    "    train_set = pd.DataFrame(train_set_x)\n",
    "    train_set['Y'] = train_set_y\n",
    "    tree = D_Tree(train_set)\n",
    "    tree.fit()\n",
    "    predict_label = tree.predict(pima_test_data)\n",
    "    cs_.append(compute_cs(cost_matrix,pima_test_label,predict_label))\n",
    "cs_ = np.array(cs_)\n",
    "print(\"under\",cs_.mean())\n",
    "print(\"under\",cs_.std())\n",
    "\n",
    "cs_ = []\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    pima_train_set,sonar_test_set = train_test_split(pima_,test_size=0.3)\n",
    "    pima_test_data = pima_test_set.iloc[:,:-1].values\n",
    "    pima_test_label = pima_test_set.iloc[:,-1].values\n",
    "    train_set_x,train_set_y = stratified_sampling(pima_train_set,cost_matrix.values,'oversampling') \n",
    "    train_set = pd.DataFrame(train_set_x)\n",
    "    train_set['Y'] = train_set_y\n",
    "    tree = D_Tree(train_set)\n",
    "    tree.fit()\n",
    "    predict_label = tree.predict(pima_test_data)\n",
    "    cs_.append(compute_cs(cost_matrix,pima_test_label,predict_label))\n",
    "cs_ = np.array(cs_)\n",
    "print(\"over\",cs_.mean())\n",
    "print(\"over\",cs_.std())\n",
    "\n",
    "cs___=[]\n",
    "for i in range(20):\n",
    "    cost_matrix = make_cost_matrix(y)\n",
    "    pima_train_set,sonar_test_set = train_test_split(pima_,test_size=0.3,random_state=42)\n",
    "    pima_test_data = pima_test_set.iloc[:,:-1].values\n",
    "    pima_test_label = pima_test_set.iloc[:,-1].values\n",
    "    tree = DecisionTreeClassifier(criterion='entropy')\n",
    "    meta = Metacost(pima_train_set,tree,cost_matrix.values,q=False)\n",
    "    model = meta.fit('Y')\n",
    "    predict_label = model.predict(pima_test_data)\n",
    "    cs___.append(compute_cs(cost_matrix,pima_test_label,predict_label))\n",
    "cs___ = np.array(cs___)\n",
    "# print(cs__)\n",
    "print(\"meta\",np.mean(cs___))\n",
    "print(\"meta\",np.std(cs___))\n",
    "\n",
    "# cs_k = []\n",
    "# for i in range(20):\n",
    "#     cost_matrix = make_cost_matrix(y)\n",
    "#     pima_train_set,pima_test_set = train_test_split(pima_,test_size=0.3,random_state=42)\n",
    "#     pima_test_data = pima_test_set.iloc[:,:-1].values\n",
    "#     pima_test_label = pima_test_set.iloc[:,-1].values\n",
    "#     knn = KNN(6,pima_train_set,50)\n",
    "    \n",
    "#     knn.fit(cost_matrix.values,n_neighbor=20,euclidean_features=[i for i in range(pima_train_set.shape[1]-1)])\n",
    "#     predict_label = knn.predict(pima_test_set)\n",
    "#     cs_k.append(compute_cs(cost_matrix,pima_test_label,predict_label))\n",
    "# cs_k = np.array(cs_k)\n",
    "# print(\"my_algorithm\",np.mean(cs_k))\n",
    "# print(\"my_algorithm\",np.std(cs_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fea44c8-cfa4-4fd8-a6b9-d6a4f42089ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
